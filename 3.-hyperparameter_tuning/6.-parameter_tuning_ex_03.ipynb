{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934c6a3e",
   "metadata": {},
   "source": [
    "# üìù Exercise M3.02\n",
    "\n",
    "The goal is to find the best set of hyperparameters which maximize the\n",
    "generalization performance on a training set.\n",
    "\n",
    "Here again with limit the size of the training set to make computation\n",
    "run faster. Feel free to increase the `train_size` value if your computer\n",
    "is powerful enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f959818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census.csv\")\n",
    "\n",
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, train_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb78cdf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this exercise, we will progressively define the classification pipeline\n",
    "and later tune its hyperparameters.\n",
    "\n",
    "Our pipeline should:\n",
    "* preprocess the categorical columns using a `OneHotEncoder` and use a\n",
    "  `StandardScaler` to normalize the numerical data.\n",
    "* use a `LogisticRegression` as a predictive model.\n",
    "\n",
    "Start by defining the columns and the preprocessing pipelines to be applied\n",
    "on each group of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553573ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols : ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "Categorical cols : ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "print(f\"Categorical cols : {categorical_columns}\")\n",
    "\n",
    "numerical_columns_selector = selector(dtype_include=int)\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "print(f\"Categorical cols : {numerical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6c3856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Write your code here.\n",
    "categorical_processor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_processor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a11a5c",
   "metadata": {},
   "source": [
    "Subsequently, create a `ColumnTransformer` to redirect the specific columns\n",
    "a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f14b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# solution\n",
    "preprocessor = ColumnTransformer(\n",
    "    [('cat_preprocessor', categorical_processor, categorical_columns),\n",
    "     ('num_preprocessor', numerical_processor, numerical_columns)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b1a45",
   "metadata": {},
   "source": [
    "Assemble the final pipeline by combining the above preprocessor\n",
    "with a logistic regression classifier. Force the maximum number of\n",
    "iterations to `10_000` to ensure that the model will converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cca5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7cea265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('cat_preprocessor',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['workclass', 'education',\n",
       "                                                   'marital-status',\n",
       "                                                   'occupation', 'relationship',\n",
       "                                                   'race', 'sex',\n",
       "                                                   'native-country']),\n",
       "                                                 ('num_preprocessor',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['age', 'capital-gain',\n",
       "                                                   'capital-loss',\n",
       "                                                   'hours-per-week'])])),\n",
       "                ('logisticregression', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=10_000))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dc1d3",
   "metadata": {},
   "source": [
    "Use `RandomizedSearchCV` with `n_iter=20` to find the best set of\n",
    "hyperparameters by tuning the following parameters of the `model`:\n",
    "\n",
    "- the parameter `C` of the `LogisticRegression` with values ranging from\n",
    "  0.001 to 10. You can use a log-uniform distribution\n",
    "  (i.e. `scipy.stats.loguniform`);\n",
    "- the parameter `with_mean` of the `StandardScaler` with possible values\n",
    "  `True` or `False`;\n",
    "- the parameter `with_std` of the `StandardScaler` with possible values\n",
    "  `True` or `False`.\n",
    "\n",
    "Once the computation has completed, print the best combination of parameters\n",
    "stored in the `best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b7cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in model.get_params().keys() if 'log' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3ae6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0653ed10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/2079720451.py\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_random_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m# model_random_search.fit(data_train, target_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "for i in range(1, 21):\n",
    "    param_distributions = {\n",
    "        'logisticregression__C': loguniform(1e-6, 1e3),\n",
    "        'standardscaler__with_mean': [True, False],\n",
    "        'standardscaler__with_std': [True, False]\n",
    "    }\n",
    "\n",
    "    model_random_search= RandomizedSearchCV(\n",
    "        model, param_distributions=param_distributions, n_iter=10,\n",
    "        cv=5, verbose=1,\n",
    "    )\n",
    "    print(model_random_search.best_params_)\n",
    "# model_random_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21e2190c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'columntransformer', 'logisticregression', 'columntransformer__n_jobs', 'columntransformer__remainder', 'columntransformer__sparse_threshold', 'columntransformer__transformer_weights', 'columntransformer__transformers', 'columntransformer__verbose', 'columntransformer__verbose_feature_names_out', 'columntransformer__cat_preprocessor', 'columntransformer__num_preprocessor', 'columntransformer__cat_preprocessor__categories', 'columntransformer__cat_preprocessor__drop', 'columntransformer__cat_preprocessor__dtype', 'columntransformer__cat_preprocessor__handle_unknown', 'columntransformer__cat_preprocessor__sparse', 'columntransformer__num_preprocessor__copy', 'columntransformer__num_preprocessor__with_mean', 'columntransformer__num_preprocessor__with_std', 'logisticregression__C', 'logisticregression__class_weight', 'logisticregression__dual', 'logisticregression__fit_intercept', 'logisticregression__intercept_scaling', 'logisticregression__l1_ratio', 'logisticregression__max_iter', 'logisticregression__multi_class', 'logisticregression__n_jobs', 'logisticregression__penalty', 'logisticregression__random_state', 'logisticregression__solver', 'logisticregression__tol', 'logisticregression__verbose', 'logisticregression__warm_start'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c4eb6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'columntransformer__num_preprocessor__with_mean': False,\n",
       " 'columntransformer__num_preprocessor__with_std': False,\n",
       " 'logisticregression__C': 0.17169565852473864}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# solution\n",
    "param_distributions = {\n",
    "    \"logisticregression__C\": loguniform(0.001, 10),\n",
    "    \"columntransformer__num_preprocessor__with_mean\": [True, False],\n",
    "    \"columntransformer__num_preprocessor__with_std\": [True, False],\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions,\n",
    "    n_iter=20, error_score=np.nan, n_jobs=2, verbose=1, random_state=1)\n",
    "model_random_search.fit(data_train, target_train)\n",
    "model_random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f858cf",
   "metadata": {},
   "source": [
    "So the best hyperparameters give a model where the features are scaled but not centered and the final model is regularized.\n",
    "\n",
    "Getting the best parameter combinations is the main outcome of the hyper-parameter optimization procedure. However it is also interesting to assess the sensitivity of the best models to the choice of those parameters. The following code, not required to answer the quiz question shows how to conduct such an interactive analysis for this this pipeline using a parallel coordinate plot using the plotly library.\n",
    "\n",
    "We could use cv_results = model_random_search.cv_results_ to make a parallel coordinate plot as we did in the previous notebook (you are more than welcome to try!). Instead we are going to load the results obtained from a similar search with many more iterations (1,000 instead of 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d51f1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.read_csv(\n",
    "    \"../figures/randomized_search_results_logistic_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7585e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_columntransformer__num_preprocessor__with_mean',\n",
       "       'param_columntransformer__num_preprocessor__with_std',\n",
       "       'param_logisticregression__C', 'params', 'split0_test_score',\n",
       "       'split1_test_score', 'split2_test_score', 'split3_test_score',\n",
       "       'split4_test_score', 'mean_test_score', 'std_test_score',\n",
       "       'rank_test_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17b6453f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_columntransformer__num_preprocessor__with_mean</th>\n",
       "      <th>param_columntransformer__num_preprocessor__with_std</th>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.380337</td>\n",
       "      <td>0.042428</td>\n",
       "      <td>0.033888</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.279554</td>\n",
       "      <td>{'columntransformer__num_preprocessor__with_me...</td>\n",
       "      <td>0.846469</td>\n",
       "      <td>0.855681</td>\n",
       "      <td>0.847492</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839222</td>\n",
       "      <td>0.848280</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.383388</td>\n",
       "      <td>0.088132</td>\n",
       "      <td>0.029121</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.254750</td>\n",
       "      <td>{'columntransformer__num_preprocessor__with_me...</td>\n",
       "      <td>0.845957</td>\n",
       "      <td>0.854657</td>\n",
       "      <td>0.848004</td>\n",
       "      <td>0.852023</td>\n",
       "      <td>0.839734</td>\n",
       "      <td>0.848075</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>0.464065</td>\n",
       "      <td>0.085474</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.247167</td>\n",
       "      <td>{'columntransformer__num_preprocessor__with_me...</td>\n",
       "      <td>0.846469</td>\n",
       "      <td>0.854657</td>\n",
       "      <td>0.848004</td>\n",
       "      <td>0.851510</td>\n",
       "      <td>0.839734</td>\n",
       "      <td>0.848075</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "926       0.380337      0.042428         0.033888        0.009012   \n",
       "282       0.383388      0.088132         0.029121        0.001475   \n",
       "787       0.464065      0.085474         0.040924        0.010010   \n",
       "\n",
       "     param_columntransformer__num_preprocessor__with_mean  \\\n",
       "926                                              False      \n",
       "282                                               True      \n",
       "787                                               True      \n",
       "\n",
       "     param_columntransformer__num_preprocessor__with_std  \\\n",
       "926                                               True     \n",
       "282                                               True     \n",
       "787                                               True     \n",
       "\n",
       "     param_logisticregression__C  \\\n",
       "926                     0.279554   \n",
       "282                     0.254750   \n",
       "787                     0.247167   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "926  {'columntransformer__num_preprocessor__with_me...           0.846469   \n",
       "282  {'columntransformer__num_preprocessor__with_me...           0.845957   \n",
       "787  {'columntransformer__num_preprocessor__with_me...           0.846469   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "926           0.855681           0.847492           0.852535   \n",
       "282           0.854657           0.848004           0.852023   \n",
       "787           0.854657           0.848004           0.851510   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "926           0.839222         0.848280        0.005636                1  \n",
       "282           0.839734         0.848075        0.005157                2  \n",
       "787           0.839734         0.848075        0.005046                3  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.sort_values('rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c93e5e",
   "metadata": {},
   "source": [
    " Selecting the best performing models (i.e. above an accuracy of ~0.845), we observe the following pattern:\n",
    "\n",
    "scaling the data is important. All the best performing models are scaling the data;\n",
    "\n",
    "centering the data does not have a strong impact. Both approaches, centering and not centering, can lead to good models;\n",
    "\n",
    "using some regularization is fine but using too much is a problem. Recall that a smaller value of C means a stronger regularization. In particular no pipeline with C lower than 0.001 can be found among the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5af246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/parameter_tuning_ex_03.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
