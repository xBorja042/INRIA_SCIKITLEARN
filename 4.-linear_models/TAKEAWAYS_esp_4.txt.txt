0.- 
* Linear models es bien porque:
- Simples, rápidos de entrenar y buena explicatividad. 
* Tienden a underfittear cuando:
Hay pocas features y muchas muestras. Se puede hacer algo con feature engineering. 
* Si se tienen muchas features van a ser difíciles de batir incluso por modelos más complejos.
Son malos cuando el problema no es linealmente separable.


A linear regression model minimizes the mean squared error on the training set. This means that the parameters obtained after the fit (i.e. coef_ and intercept_) are the optimal parameters that minimizes the mean squared error. In other words, any other choice of parameters will yield a model with a higher mean squared error on the training set.

However, the mean squared error is difficult to interpret. The mean absolute error is more intuitive since it provides an error in the same unit as the one of the target.

1.- Pueden overfittear:
- n_samples << n_features
- Muchas features con poca info.

2.- Para arreglar el overfitting:
  Regularizacion
  RIDGE --> Intenta llevar los coeficientes a 0. Este efecto se controla con el parámetro alfa. Se puede obtener el buen alfa ya sea mediante GridSearch o utilizando RidgeCv. Cuanto más grande, mayor regularización.

 3.- Logistic Regs on scikit learn ya están regularizados. Se controla con el parámetro C, que a más C menos regularización. 

 4.- Regularización fuerza a los pesos a que no tengan tanta desviación estándard. Si usamos regularización para reducir el overfitting va a ser importante escalar las features para que todas puedan contribuir de igual forma y volver a ser reescaladas con la regularización. Además este escalado podría ser importante para solvers basados en cálculo de gradiente.
 Si aumentamos alfa en ridge decrecerán los valores de los pesos. Hay que tener cuidado al escalar variables categóricas porque si se han codificado por frecuencia quizá haya veces que dividamos casi por /~0. 

 Va a ser importante tunear este alfa con Xvalid también.


 For linear models, regularization refers to controlling to size the weights. This is especially important when the model is fit on a data with either a small number of training samples or a large number of features, especially if some of those features are not very related to the target variable.

TEST
Regularization makes the linear models more constraints and can augment underfitting rather than reduce it. If the problem is not linearly separable, one should instead try to engineer new predictive features as we will explain later in this module.

Fitting linear models that are robust to outliers is a complex issue that cannot be solved by simple regularization alone. Please refer to the scikit-learn documentation robust regression if you are interested.

alpha is not changed during fit, only coef_ and intercept_ change when training the model.

c) is wrong: one should never choose any hyper-parameters based on the test set: this will overestimate the generalization performance of the model.

Here are some reasons for scaling features:

When the original feature values have widely difference natural scales, fitting a linear model on the raw features can cause the learning procedure to fail because of numerical problems. Furthermore, if the model is regularized, the regularization would have comparitively little impact on the features with the largest scales because those features would anyway be assigned comparatively smaller weights when training the model. If many such features are not predictive, the benefit of using regularization would therefore be reduced.
Scaling is not always necessary, for instance if the features values naturally vary with similar ranges by default.
Since scaling features has a impact on the relative magnitude of the weights of the trained model, deciding to scale the features or not can change the optimal value of the regularization parameter of a linear model when this parameter is tuned to improve a generalization metric estimated using cross-validation.
Many models such as logistic regression use numerical solvers (based on gradient descent) to find their optimal parameters. These solvers often converge much faster when the features are scaled.
Note that the value of the regularization parameter can also impact the speed of convergence of gradient descent solvers.


Wrap-up
In this module, we saw that:

the predictions of a linear model depend on a weighted sum of the values of the input features added to an intercept parameter;

fitting a linear model consists in adjusting both the weight coefficients and the intercept to minimize the prediction errors on the training set;

to train linear models successfully it is often required to scale the input features approximately to the same dynamic range;

regularization can be used to reduce over-fitting: weight coefficients are constrained to stay small when fitting;

the regularization hyperparameter needs to be fine-tuned by cross-validation for each new machine learning problem and dataset;

linear models can be used on problems where the target variable is not linearly related to the input features but this requires extra feature engineering work to transform the data in order to avoid under-fitting.