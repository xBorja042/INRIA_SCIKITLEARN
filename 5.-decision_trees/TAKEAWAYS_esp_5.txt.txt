Decision Tree

- Funcionan bien en tabular data porque no necesitan estar las featuras en la misma escala. Se van ajustando variable a variable.

- Pueden underfittear si el número de splits es insuficiente. Y al revés, ovefittear si nos pasamos. Se refleja muy bien en la gráfica de la ~. Por eso usamos pruning y se talan los árboles. Hay que elegir bien la máximo profundidad o leaf nodes para controlar el tradeoff.

- When fitting a decision tree regressor in scikit-learn, the predicted values on a leaf corresponds to: The mean of the training samples at this node 

- The predictions of a tree regressor correspond to:
a piecewise-constant function
When predicting with a decision tree regressor, we will predict the mean of the training samples at a leaf. This value is indeed a constant.

- The max_depth hyperparameter controls the overall complexity of the tree. This parameter is adequate under the assumption that a tree is built symmetrically. However, there is no guarantee that a tree will be symmetrical. Indeed, optimal generalization performance could be reached by growing some of the branches deeper than some others.

We have built a dataset where we will illustrate this asymmetry. We will generate a dataset composed of 2 subsets: one subset where a clear separation should be found by the tree and another subset where samples from both classes will be mixed. It implies that a decision tree will need more splits to classify properly samples from the second subset than from the first subset.

- How should you choose the maximum depth of a decision tree?

 a) choosing the depth maximizing the score on a validation set with a cross-validation, with a grid-search for instance 