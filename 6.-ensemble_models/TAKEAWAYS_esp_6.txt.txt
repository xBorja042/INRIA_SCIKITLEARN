1.- BAGGING
a) BOOTSTRAPPING: Se hacen subsets y se fitean un modelo por cada subset. 
b) AGGREGATING: Suma de los votos de los modelos.

A pesar de que los modelos por separado cometan overfitting, la suma de los modelos hará menos overfitting.
En regresión el procedimiento es similar y el overfit también será diferente para cada árbol. 

Los RANDOM TREES son randoms ya que se utiliza un set de features aleatorio lo que decorrelará el error de predicción

Without searching for optimal hyperparameters, the overall generalization
performance of the bagging regressor is better than a single decision tree.
In addition, the computational cost is reduced in comparison of seeking
for the optimal hyperparameters.

This shows the motivation behind the use of an ensemble learner: it gives a
relatively good baseline with decent generalization performance without any
parameter tuning.

"Bagging" stands for Bootstrap AGGregatING. It uses bootstrap resampling (random sampling with replacement) to learn several models on random variations of the training set. At predict time, the predictions of each learner are aggregated to give the final predictions.

First, we will generate a simple synthetic dataset to get insights regarding bootstraping.

Bootstrap resampling¶
A bootstrap sample corresponds to a resampling with replacement, of the original dataset, a sample that is the same size as the original dataset. Thus, the bootstrap sample will contain some data points several times while some of the original data points will not be present.

Observe that the 3 variations all share common points with the original dataset. Some of the points are randomly resampled several times and appear as darker blue circles.

The 3 generated bootstrap samples are all different from the original dataset and from each other. To confirm this intuition, we can check the number of unique samples in the bootstrap samples.

It is possible to access the internal models of the ensemble stored as a Python list in the bagged_trees.estimators_ attribute after fitting.


Random forests¶
In this notebook, we will present the random forest models and show the differences with the bagging ensembles.

Random forests are a popular model in machine learning. They are a modification of the bagging algorithm. In bagging, any classifier or regressor can be used. In random forests, the base classifier or regressor is always a decision tree.

Random forests have another particularity: when training a tree, the search for the best split is done only on a subset of the original features taken at random. The random subsets are different for each split node. The goal is to inject additional randomization into the learning procedure to try to decorrelate the prediction errors of the individual trees.

Therefore, random forests are using randomization on both axes of the data matrix:

by bootstrapping samples for each tree in the forest;
randomly selecting a subset of features at each node of the tree.


Q1
Bagging corresponds to bootstrap-aggregating. Therefore, we resample the dataset with replacement several times to get several bootstrap samples. Each bootstrap sample will be used to train a model. The predictions of each model are aggregated via a vote or an average to give a final prediction for the ensemble as a whole.

It is also possible to randomly resample the features using max_features with a value in the (0.0, 1.0) range but this is disabled by default (max_features=1.0 meaning all the features are used).

For feature resampling, it is possible to choose between with replacement (bootstrap_features=True) or without replacement (bootstrap_features=False, the default).


In the context of a classification problem, what are the differences between a bagging classifier and a random forest classifier:

 a) in a random forest, the base model is always a decision tree c) in a random forest, a random resampling is performed both over features as well as over samples

A random forest is a bagging ensemble using trees. It includes an additional random resampling on the features at each split node in the trees.


2.- BOOSTING. 

Tradicional 
- Errores son re-pesados en cada paso.
- Se puede usar cualquier modelo que acepte sample_weight

vs Gradient Boosting
- Cada modelo predice los errores de cada modelo previo.
- En sklearn se usan árboles como base model.

Mejor usar HistGradientBoosting que discretiza variables numéricas.


			Bagging 				Boosting
fit trees independientlly.		fit trees sequentially
each deep tree overfits 		eache shallow tree underfits
averaging the tree predictions	sequentially adding trees reduces underfitting
reduces overfitting				


Scikit-learn provides specific classes which are even more optimized for large dataset, called HistGradientBoostingClassifier and HistGradientBoostingRegressor. Each feature in the dataset data is first binned by computing histograms, which are later used to evaluate the potential splits. The number of splits to evaluate is then much smaller. This algorithm becomes much more efficient than gradient boosting when the dataset has over 10,000 samples.


HyperParam Tuning --> Random forest

The main parameter to tune for random forest is the n_estimators parameter. In general, the more trees in the forest, the better the generalization performance will be. However, it will slow down the fitting and prediction time. The goal is to balance computing time and generalization performance when setting the number of estimators when putting such learner in production.

Then, we could also tune a parameter that controls the depth of each tree in the forest. Two parameters are important for this: max_depth and max_leaf_nodes. They differ in the way they control the tree structure. Indeed, max_depth will enforce to have a more symmetric tree, while max_leaf_nodes does not impose such constraint.

Be aware that with random forest, trees are generally deep since we are seeking to overfit each tree on each bootstrap sample because this will be mitigated by combining them altogether. Assembling underfitted trees (i.e. shallow trees) might also lead to an underfitted forest.


HyperParam Tuning --> Gradient-boosting decision trees
For gradient-boosting, parameters are coupled, so we cannot set the parameters one after the other anymore. The important parameters are n_estimators, learning_rate, and max_depth or max_leaf_nodes (as previously discussed random forest).

Let's first discuss the max_depth (or max_leaf_nodes) parameter. We saw in the section on gradient-boosting that the algorithm fits the error of the previous tree in the ensemble. Thus, fitting fully grown trees would be detrimental. Indeed, the first tree of the ensemble would perfectly fit (overfit) the data and thus no subsequent tree would be required, since there would be no residuals. Therefore, the tree used in gradient-boosting should have a low depth, typically between 3 to 8 levels, or few leaves ( 23=8  to  28=256 ). Having very weak learners at each step will help reducing overfitting.

With this consideration in mind, the deeper the trees, the faster the residuals will be corrected and less learners are required. Therefore, n_estimators should be increased if max_depth is lower.

Finally, we have overlooked the impact of the learning_rate parameter until now. When fitting the residuals, we would like the tree to try to correct all possible errors or only a fraction of them. The learning-rate allows you to control this behaviour. A small learning-rate value would only correct the residuals of very few samples. If a large learning-rate is set (e.g., 1), we would fit the residuals of all samples. So, with a very low learning-rate, we will need more estimators to correct the overall error. However, a too large learning-rate tends to obtain an overfitted ensemble, similar to having a too large tree depth.