1.- BAGGING
a) BOOTSTRAPPING: Se hacen subsets y se fitean un modelo por cada subset. 
b) AGGREGATING: Suma de los votos de los modelos.

A pesar de que los modelos por separado cometan overfitting, la suma de los modelos hará menos overfitting.
En regresión el procedimiento es similar y el overfit también será diferente para cada árbol. 

Los RANDOM TREES son randoms ya que se utiliza un set de features aleatorio lo que decorrelará el error de predicción

Without searching for optimal hyperparameters, the overall generalization
performance of the bagging regressor is better than a single decision tree.
In addition, the computational cost is reduced in comparison of seeking
for the optimal hyperparameters.

This shows the motivation behind the use of an ensemble learner: it gives a
relatively good baseline with decent generalization performance without any
parameter tuning.

"Bagging" stands for Bootstrap AGGregatING. It uses bootstrap resampling (random sampling with replacement) to learn several models on random variations of the training set. At predict time, the predictions of each learner are aggregated to give the final predictions.

First, we will generate a simple synthetic dataset to get insights regarding bootstraping.

Bootstrap resampling¶
A bootstrap sample corresponds to a resampling with replacement, of the original dataset, a sample that is the same size as the original dataset. Thus, the bootstrap sample will contain some data points several times while some of the original data points will not be present.

Observe that the 3 variations all share common points with the original dataset. Some of the points are randomly resampled several times and appear as darker blue circles.

The 3 generated bootstrap samples are all different from the original dataset and from each other. To confirm this intuition, we can check the number of unique samples in the bootstrap samples.

It is possible to access the internal models of the ensemble stored as a Python list in the bagged_trees.estimators_ attribute after fitting.


Random forests¶
In this notebook, we will present the random forest models and show the differences with the bagging ensembles.

Random forests are a popular model in machine learning. They are a modification of the bagging algorithm. In bagging, any classifier or regressor can be used. In random forests, the base classifier or regressor is always a decision tree.

Random forests have another particularity: when training a tree, the search for the best split is done only on a subset of the original features taken at random. The random subsets are different for each split node. The goal is to inject additional randomization into the learning procedure to try to decorrelate the prediction errors of the individual trees.

Therefore, random forests are using randomization on both axes of the data matrix:

by bootstrapping samples for each tree in the forest;
randomly selecting a subset of features at each node of the tree.


Q1
Bagging corresponds to bootstrap-aggregating. Therefore, we resample the dataset with replacement several times to get several bootstrap samples. Each bootstrap sample will be used to train a model. The predictions of each model are aggregated via a vote or an average to give a final prediction for the ensemble as a whole.

It is also possible to randomly resample the features using max_features with a value in the (0.0, 1.0) range but this is disabled by default (max_features=1.0 meaning all the features are used).

For feature resampling, it is possible to choose between with replacement (bootstrap_features=True) or without replacement (bootstrap_features=False, the default).


In the context of a classification problem, what are the differences between a bagging classifier and a random forest classifier:

 a) in a random forest, the base model is always a decision tree c) in a random forest, a random resampling is performed both over features as well as over samples

A random forest is a bagging ensemble using trees. It includes an additional random resampling on the features at each split node in the trees.


2.- BOOSTING. 

Tradicional 
- Errores son re-pesados en cada paso.
- Se puede usar cualquier modelo que acepte sample_weight

vs Gradient Boosting
- Cada modelo predice los errores de cada modelo previo.
- En sklearn se usan árboles como base model.

Mejor usar HistGradientBoosting que discretiza variables numéricas.


			Bagging 				Boosting
fit trees independientlly.		fit trees sequentially
each deep tree overfits 		eache shallow tree underfits
averaging the tree predictions	sequentially adding trees reduces underfitting
reduces overfitting				